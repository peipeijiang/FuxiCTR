#!/usr/bin/env python
"""
ä¸šç•Œçº§å¤šä»»åŠ¡ç‰¹å¾ç­›é€‰å®Œæ•´æµç¨‹
Industry-Standard Multi-Task Feature Selection Pipeline

å‚è€ƒæ¥æº:
- MIT Press (2025): Multitask Learning 1997-2024: Regularization and Optimization
- Cambridge (2024): Multitask feature selection with LASSO
- Springer (2025): Deep multi-task learning review
- ä¸šç•Œå®è·µ: é˜¿é‡Œã€å­—èŠ‚ã€è…¾è®¯æ¨èç³»ç»Ÿ

æµç¨‹æ¦‚è¿°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: æ•°æ®è´¨é‡æ£€æŸ¥ (Data Quality Check)                      â”‚
â”‚  Stage 2: æ•°æ®æ³„éœ²æ£€æµ‹ (Data Leakage Detection)                   â”‚
â”‚  Stage 4: å¤šä»»åŠ¡ç‰¹å¼‚æ€§åˆ†æ (Multi-Task Specific Analysis)        â”‚
â”‚  Stage 5: Wrapper/Embeddedæ–¹æ³• (Model-Based Selection) â”‚  Stage 3: åŸºç¡€ç‰¹å¾ç­›é€‰ (Filter Methods)                          â”‚
         â”‚
â”‚  Stage 6: ç‰¹å¾ç¨³å®šæ€§éªŒè¯ (Stability Validation)                  â”‚
â”‚  Stage 7: ä¸šåŠ¡é€»è¾‘å®¡æŸ¥ (Domain Review)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½¿ç”¨æ–¹æ³•:
    python multi_task_feature_selection_pipeline.py --stage all
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')


class MultiTaskFeatureSelectionPipeline:
    """å¤šä»»åŠ¡ç‰¹å¾ç­›é€‰æµæ°´çº¿"""

    def __init__(self, data_path: str, label_cols: List[str], output_dir: str):
        self.data_path = data_path
        self.label_cols = label_cols
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½æ•°æ®
        print(f"Loading data from: {data_path}")
        self.df = pd.read_parquet(data_path)
        print(f"Data shape: {self.df.shape}")

        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.results = {
            'stage1_data_quality': {},
            'stage2_leakage': {},
            'stage3_filter': {},
            'stage4_multitask': {},
            'stage5_model': {},
            'stage6_stability': {},
            'stage7_domain': {},
            'final_features': []
        }

    # ============================================================
    # Stage 1: æ•°æ®è´¨é‡æ£€æŸ¥
    # ============================================================
    def stage1_data_quality_check(self) -> Dict:
        """
        é˜¶æ®µ1: æ•°æ®è´¨é‡æ£€æŸ¥

        æ£€æŸ¥é¡¹:
        1. ç¼ºå¤±å€¼ç‡ > 50% â†’ ç§»é™¤
        2. å¸¸æ•°ç‰¹å¾ (unique=1) â†’ ç§»é™¤
        3. é«˜åŸºæ•°ç‰¹å¾ (>10000) â†’ éœ€è¦ç¼–ç 
        4. å”¯ä¸€å€¼ç‰¹å¾ (unique=æ ·æœ¬æ•°) â†’ ç§»é™¤

        ä¸šç•Œæ ‡å‡†: å‚è€ƒé˜¿é‡Œã€å­—èŠ‚æ¨èç³»ç»Ÿç‰¹å¾å·¥ç¨‹è§„èŒƒ
        """
        print("\n" + "="*80)
        print("STAGE 1: DATA QUALITY CHECK")
        print("="*80)

        remove_features = set()
        warning_features = {}

        # æ–°å¢ï¼šæŒ‰ç­›é€‰åŸå› åˆ†ç»„
        removed_by_reason = {
            'high_missing': [],       # é«˜ç¼ºå¤±ç‡
            'constant_features': [],  # å¸¸æ•°ç‰¹å¾
            'zero_variance': [],      # é›¶æ–¹å·®
        }

        # ç‰¹å¾åˆ—è¡¨
        feature_cols = [col for col in self.df.columns if col not in self.label_cols]

        # 1. ç¼ºå¤±å€¼æ£€æŸ¥
        print("\n[1/4] Missing Value Check...")
        for col in feature_cols:
            missing_rate = self.df[col].isna().sum() / len(self.df)

            if missing_rate > 0.5:
                remove_features.add(col)
                removed_by_reason['high_missing'].append({
                    'feature': col,
                    'missing_rate': missing_rate
                })
                print(f"  âŒ REMOVE: {col} (missing={missing_rate:.1%})")
            elif missing_rate > 0.3:
                warning_features[col] = warning_features.get(col, []) + ['high_missing']
                print(f"  âš ï¸  WARNING: {col} (missing={missing_rate:.1%})")

        # 2. å¸¸æ•°ç‰¹å¾æ£€æŸ¥
        print("\n[2/4] Constant Feature Check...")
        for col in self.df.select_dtypes(include=[np.number]).columns:
            if col in self.label_cols:
                continue
            unique_count = self.df[col].nunique()

            if unique_count <= 1:
                remove_features.add(col)
                removed_by_reason['constant_features'].append({
                    'feature': col,
                    'unique_count': unique_count
                })
                print(f"  âŒ REMOVE: {col} (unique={unique_count})")
            elif unique_count == len(self.df):
                # IDç‰¹å¾ï¼Œéœ€è¦ç¡®è®¤
                warning_features[col] = warning_features.get(col, []) + ['id_like']
                print(f"  âš ï¸  WARNING: {col} (unique={unique_count}, possibly ID)")

        # 3. é«˜åŸºæ•°ç‰¹å¾æ£€æŸ¥
        print("\n[3/4] High Cardinality Check...")
        for col in self.df.columns:
            if col in self.label_cols:
                continue
            # è·³è¿‡åºåˆ—ç±»å‹åˆ—ï¼ˆlist/arrayï¼‰
            if self.df[col].dtype == 'object':
                # æ£€æŸ¥æ˜¯å¦æ˜¯åºåˆ—ç±»å‹
                sample = self.df[col].dropna().iloc[0] if len(self.df[col].dropna()) > 0 else None
                if isinstance(sample, (list, np.ndarray)):
                    print(f"  â„¹ï¸  {col}: sequence feature, skipping cardinality check")
                    continue

            try:
                cardinality = self.df[col].nunique()
            except TypeError:
                # æ— æ³•è®¡ç®—åŸºæ•°çš„åˆ—
                continue

            if cardinality > 10000:
                warning_features[col] = warning_features.get(col, []) + ['high_cardinality']
                print(f"  âš ï¸  WARNING: {col} (cardinality={cardinality})")

        # 4. é›¶æ–¹å·®ç‰¹å¾
        print("\n[4/4] Zero Variance Check...")
        for col in self.df.select_dtypes(include=[np.number]).columns:
            if col in self.label_cols:
                continue
            if self.df[col].std() == 0:
                remove_features.add(col)
                removed_by_reason['zero_variance'].append({
                    'feature': col
                })
                print(f"  âŒ REMOVE: {col} (zero variance)")

        self.results['stage1_data_quality'] = {
            'remove': list(remove_features),
            'warning': warning_features,
            'removed_by_reason': removed_by_reason
        }

        print(f"\nâœ“ Stage 1 Complete: {len(remove_features)} features to remove")

        return {
            'remove': list(remove_features),
            'warning': warning_features,
            'removed_by_reason': removed_by_reason
        }

    # ============================================================
    # è¾…åŠ©æ–¹æ³•: ç‰¹å¾ç±»å‹æ£€æµ‹
    # ============================================================
    def _is_sequence_feature(self, col: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåºåˆ—ç‰¹å¾ï¼ˆåŒ…å«listæˆ–ndarrayï¼‰"""
        if self.df[col].dtype != 'object':
            return False
        sample = self.df[col].dropna().iloc[0] if len(self.df[col].dropna()) > 0 else None
        return isinstance(sample, (list, np.ndarray))

    def _get_feature_cardinality(self, col: str) -> int:
        """è·å–ç‰¹å¾åŸºæ•°ï¼ˆå”¯ä¸€å€¼æ•°é‡ï¼‰"""
        return self.df[col].nunique()

    def _is_categorical_feature(self, col: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºç±»åˆ«ç‰¹å¾ï¼ˆéæ•°å€¼ã€éåºåˆ—ï¼‰"""
        if self._is_sequence_feature(col):
            return False
        if pd.api.types.is_numeric_dtype(self.df[col]):
            return False
        return True

    def _calculate_woe_iv_categorical(self, feature: pd.Series, label: pd.Series) -> float:
        """
        è®¡ç®—ç±»åˆ«ç‰¹å¾çš„WoEå’ŒIVï¼ˆç”¨äºäºŒåˆ†ç±»ï¼‰

        å‚è€ƒæ–‡çŒ®:
        - Siddiqi, N. (2006). "Credit Risk Scorecards"
        - Industry standard in financial risk control

        IVé˜ˆå€¼æ ‡å‡†:
        - IV < 0.02:  æ— é¢„æµ‹èƒ½åŠ›ï¼Œç§»é™¤
        - 0.02-0.1:  å¼±é¢„æµ‹èƒ½åŠ›
        - 0.1-0.3:   ä¸­ç­‰é¢„æµ‹èƒ½åŠ›
        - 0.3-0.5:   å¼ºé¢„æµ‹èƒ½åŠ›
        - IV > 0.5:  å¯ç–‘ï¼ˆå¯èƒ½æ•°æ®æ³„éœ²ï¼‰

        Args:
            feature: ç±»åˆ«ç‰¹å¾æ•°æ®
            label: æ ‡ç­¾æ•°æ® (0/1)

        Returns:
            IVå€¼
        """
        try:
            # åˆå¹¶æ•°æ®ï¼Œç§»é™¤NaN
            df_temp = pd.DataFrame({'feature': feature, 'label': label}).dropna()

            if len(df_temp) < 100:  # æ ·æœ¬å¤ªå°‘
                return 0.0

            # ç»Ÿè®¡æ€»çš„å¥½åæ ·æœ¬æ•°
            total_good = (df_temp['label'] == 1).sum()
            total_bad = (df_temp['label'] == 0).sum()

            if total_good == 0 or total_bad == 0:
                return 0.0

            iv_sum = 0

            # å¯¹æ¯ä¸ªç±»åˆ«è®¡ç®— WoE å’Œ IV
            for category in df_temp['feature'].unique():
                df_cat = df_temp[df_temp['feature'] == category]
                good = df_cat['label'].sum()
                bad = df_cat.shape[0] - good

                # è·³è¿‡ç©ºç±»åˆ«
                if good == 0 or bad == 0:
                    continue

                # è®¡ç®—åˆ†å¸ƒ
                dist_good = good / total_good
                dist_bad = bad / total_bad

                # å¹³æ»‘å¤„ç†ï¼Œé¿å…log(0)
                dist_good = max(dist_good, 0.0001)
                dist_bad = max(dist_bad, 0.0001)

                # è®¡ç®— WoE
                woe = np.log(dist_good / dist_bad)

                # è®¡ç®— IV
                iv = (dist_good - dist_bad) * woe
                iv_sum += iv

            return float(iv_sum)

        except Exception as e:
            return 0.0

    # ============================================================
    # Stage 2: æ•°æ®æ³„éœ²æ£€æµ‹
    # ============================================================
    def stage2_leakage_detection(self, threshold: float = 0.8) -> Dict:
        """
        é˜¶æ®µ2: æ•°æ®æ³„éœ²æ£€æµ‹

        æ–¹æ³•:
        1. åŸºäºå‘½åè§„åˆ™: reportmodel_*, *_rate, *_converate ç­‰æ˜æ˜¾æ³„éœ²
        2. åŸºäºç›¸å…³æ€§: ä¸ä»»ä¸€labelç›¸å…³æ€§ > threshold
        3. åŸºäºä¸šåŠ¡é€»è¾‘: ç‰¹å¾è®¡ç®—ä¾èµ–æœªæ¥ä¿¡æ¯

        å‚è€ƒ: Cambridge (2024) - Feature selection with LASSO
        """
        print("\n" + "="*80)
        print("STAGE 2: DATA LEAKAGE DETECTION")
        print("="*80)

        leakage_features = set()
        suspicious_features = set()

        feature_cols = [col for col in self.df.columns if col not in self.label_cols]

        # 1. åŸºäºå‘½åè§„åˆ™
        print("\n[1/3] Rule-Based Detection (Naming Patterns)...")
        leakage_patterns = [
            'reportmodel_',
            '_regisconverate_',
            '_applyconverate_',
            '_creditconverate_',
            '_rate_cnt',
            '_roi_cnt',
            '_clickrate_cnt',
            '_pricerate_cnt',
        ]

        for col in feature_cols:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in leakage_patterns):
                leakage_features.add(col)
                print(f"  âš ï¸  LEAKAGE: {col} (matched naming pattern)")

        # 2. åŸºäºç›¸å…³æ€§åˆ†æ
        print("\n[2/3] Correlation-Based Detection...")

        # æ•°å€¼ç‰¹å¾ç›¸å…³æ€§
        numeric_features = [col for col in feature_cols
                          if pd.api.types.is_numeric_dtype(self.df[col])]

        from scipy.stats import pointbiserialr

        for col in numeric_features:
            if col in leakage_features:
                continue

            feature_data = self.df[col]
            max_abs_corr = 0

            for label in self.label_cols:
                label_data = self.df[label]

                # è¿‡æ»¤NaN
                valid_mask = ~(feature_data.isna() | label_data.isna())
                if valid_mask.sum() < 1000:
                    continue

                try:
                    corr, _ = pointbiserialr(
                        label_data[valid_mask].values,
                        feature_data[valid_mask].values
                    )

                    if abs(corr) > max_abs_corr:
                        max_abs_corr = abs(corr)
                except:
                    pass

            if max_abs_corr > threshold:
                leakage_features.add(col)
                print(f"  ğŸš¨ LEAKAGE: {col} (max_corr={max_abs_corr:.4f})")
            elif max_abs_corr > 0.6:
                suspicious_features.add(col)
                print(f"  âš ï¸  SUSPICIOUS: {col} (max_corr={max_abs_corr:.4f})")

        # 3. ç‰¹æ®Šç‰¹å¾æ£€æŸ¥
        print("\n[3/3] Special Feature Check...")

        # mymodel_dk_* ç‰¹å¾ - å¯èƒ½æ˜¯æ¨¡å‹ç›¸å…³
        mymodel_features = [col for col in numeric_features if col.startswith('mymodel_dk_')]
        if mymodel_features:
            print(f"  â„¹ï¸  Found {len(mymodel_features)} mymodel_dk_* features")
            print(f"     These need domain expert review")

        self.results['stage2_leakage'] = {
            'leakage': list(leakage_features),
            'suspicious': list(suspicious_features)
        }

        print(f"\nâœ“ Stage 2 Complete: {len(leakage_features)} leakage features")

        return {'leakage': list(leakage_features), 'suspicious': list(suspicious_features)}

    def _calculate_iv(self, feature: pd.Series, label: pd.Series, n_bins: int = 10) -> float:
        """
        è®¡ç®—ç‰¹å¾çš„ä¿¡æ¯å€¼ (Information Value)

        å‚è€ƒæ–‡çŒ®:
        - Siddiqi, N. (2006). "Credit Risk Scorecards"
        - Industry standard in financial risk control

        IVé˜ˆå€¼æ ‡å‡†:
        - IV < 0.02:  æ— é¢„æµ‹èƒ½åŠ›ï¼Œç§»é™¤
        - 0.02-0.1:  å¼±é¢„æµ‹èƒ½åŠ›
        - 0.1-0.3:   ä¸­ç­‰é¢„æµ‹èƒ½åŠ›
        - 0.3-0.5:   å¼ºé¢„æµ‹èƒ½åŠ›
        - IV > 0.5:  å¯ç–‘ï¼ˆå¯èƒ½æ•°æ®æ³„éœ²ï¼‰ï¼Œæå–ä¸ºä¸šåŠ¡è§„åˆ™

        Args:
            feature: ç‰¹å¾æ•°æ®
            label: æ ‡ç­¾æ•°æ® (0/1)
            n_bins: åˆ†ç®±æ•°é‡ (é»˜è®¤10)

        Returns:
            IVå€¼
        """
        try:
            # åˆå¹¶æ•°æ®ï¼Œç§»é™¤NaN
            df_temp = pd.DataFrame({'feature': feature, 'label': label}).dropna()

            if len(df_temp) < 100:  # æ ·æœ¬å¤ªå°‘
                return 0.0

            # åˆ†ç®± (quantile-based)
            df_temp['bin'] = pd.qcut(df_temp['feature'], q=n_bins, duplicates='drop')

            # ç»Ÿè®¡æ¯ä¸ªç®±çš„å¥½åæ ·æœ¬æ•°
            stats = df_temp.groupby('bin').agg({
                'label': ['count', 'sum']
            }).reset_index()

            stats.columns = ['bin', 'total', 'bad']
            stats['good'] = stats['total'] - stats['bad']

            # è®¡ç®—æ€»çš„å¥½åæ ·æœ¬æ•°
            total_good = stats['good'].sum()
            total_bad = stats['bad'].sum()

            if total_good == 0 or total_bad == 0:
                return 0.0

            # è®¡ç®—æ¯ä¸ªç®±çš„åˆ†å¸ƒ
            stats['dist_good'] = stats['good'] / total_good
            stats['dist_bad'] = stats['bad'] / total_bad

            # å¹³æ»‘å¤„ç†ï¼Œé¿å…log(0)
            smoothing = 0.5
            stats['dist_good'] = stats['dist_good'].replace(0, smoothing / total_good)
            stats['dist_bad'] = stats['dist_bad'].replace(0, smoothing / total_bad)

            # è®¡ç®—WOE (Weight of Evidence)
            stats['woe'] = np.log(stats['dist_good'] / stats['dist_bad'])

            # è®¡ç®—IV
            stats['iv'] = (stats['dist_good'] - stats['dist_bad']) * stats['woe']

            iv_value = stats['iv'].sum()

            return float(iv_value)

        except Exception as e:
            return 0.0

    # ============================================================
    # Stage 3: åŸºç¡€ç‰¹å¾ç­›é€‰ (Filter Methods)
    # ============================================================
    def stage3_filter_methods(self, features: List[str]) -> Dict:
        """
        é˜¶æ®µ3: Filteræ–¹æ³•ç‰¹å¾ç­›é€‰ï¼ˆæ”¯æŒæ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾ï¼‰

        æ–¹æ³•:
        ã€æ•°å€¼ç‰¹å¾ã€‘:
        1. æ–¹å·®é˜ˆå€¼ (Variance Threshold) - ç§»é™¤ä½æ–¹å·®ç‰¹å¾
        2. ç›¸å…³æ€§å†—ä½™ (Correlation Redundancy) - ç§»é™¤é«˜ç›¸å…³å†—ä½™ç‰¹å¾
        3. å•ç‰¹å¾æ€§èƒ½ (Univariate Performance) - è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é¢„æµ‹èƒ½åŠ›
        4. ä¿¡æ¯å€¼ (Information Value) - é£æ§è¡Œä¸šæ ‡å‡†æ–¹æ³• â­

        ã€ç±»åˆ«ç‰¹å¾ã€‘:
        5. WoE + IV (Weight of Evidence + Information Value) - é£æ§è¡Œä¸šæ ‡å‡† â­

        å‚è€ƒæ–‡çŒ®:
        - Variance Threshold: Saeys et al. (2007), Bioinformatics (PMC)
        - Correlation Redundancy: Guyon & Elisseeff (2003), JMLR
        - IV (Information Value): Siddiqi (2006), "Credit Risk Scorecards", Wiley
        """
        print("\n" + "="*80)
        print("STAGE 3: FILTER METHODS (Numeric + Categorical)")
        print("="*80)

        if features:
            candidate_features = [f for f in features if f in self.df.columns]
        else:
            candidate_features = [col for col in self.df.columns if col not in self.label_cols]

        # 1. ç‰¹å¾åˆ†ç±»
        sequence_features = []
        categorical_features = []
        numeric_features = []

        for col in candidate_features:
            if self._is_sequence_feature(col):
                sequence_features.append(col)
            elif pd.api.types.is_numeric_dtype(self.df[col]):
                numeric_features.append(col)
            else:
                categorical_features.append(col)

        print(f"\n[Feature Type Distribution]")
        print(f"  Numeric: {len(numeric_features)}")
        print(f"  Categorical: {len(categorical_features)}")
        print(f"  Sequence (skipped): {len(sequence_features)}")

        remove_features = set()
        feature_iv_scores = {}
        feature_iv_scores_by_task = {}
        feature_scores_by_task = {}
        # Multi-task aggregation: prefer features that are both useful and stable across tasks.
        iv_multitask_alpha = 0.3
        iv_threshold = 0.02

        # æ–°å¢ï¼šæŒ‰ç­›é€‰åŸå› åˆ†ç»„
        removed_by_reason = {
            'variance_threshold': [],      # æ–¹å·®é˜ˆå€¼
            'correlation_redundancy': [],  # ç›¸å…³æ€§å†—ä½™
            'low_univariate_auc': [],      # ä½å•å˜é‡AUC
            'low_iv_numeric': [],          # æ•°å€¼ç‰¹å¾ä½IV
            'low_iv_categorical': [],      # ç±»åˆ«ç‰¹å¾ä½IV
            'correlation_pairs': []        # ç›¸å…³æ€§ç‰¹å¾å¯¹
        }

        # ============================================================
        # æ•°å€¼ç‰¹å¾ç­›é€‰
        # ============================================================
        print("\n" + "="*80)
        print("[NUMERIC FEATURES FILTERING]")
        print("="*80)

        if numeric_features:
            # 1. æ–¹å·®é˜ˆå€¼
            print("\n[1/4] Variance Threshold...")
            variance_threshold = 0.01  # ä¸šç•Œæ ‡å‡†å€¼

            for col in numeric_features:
                var = self.df[col].var()
                if var < variance_threshold:
                    remove_features.add(col)
                    removed_by_reason['variance_threshold'].append({
                        'feature': col,
                        'variance': var
                    })
                    print(f"  âŒ REMOVE: {col} (variance={var:.6f})")

            # 2. å†—ä½™ç‰¹å¾æ£€æµ‹ (é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹)
            print("\n[2/4] Redundancy Check (Correlation > 0.95)...")

            # è®¡ç®—ç‰¹å¾é—´ç›¸å…³æ€§çŸ©é˜µ
            valid_numeric = [f for f in numeric_features if f not in remove_features]
            if len(valid_numeric) > 0:
                corr_matrix = self.df[valid_numeric].corr().abs()

                # æ‰¾å‡ºé«˜ç›¸å…³ç‰¹å¾å¯¹
                high_corr_pairs = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if corr_val > 0.95:
                            feat1, feat2 = corr_matrix.columns[i], corr_matrix.columns[j]
                            high_corr_pairs.append((feat1, feat2, corr_val))

                # ä¿ç•™ç›¸å…³æ€§è¾ƒé«˜çš„ç‰¹å¾ï¼Œç§»é™¤å¦ä¸€ä¸ª
                redundant = set()
                for feat1, feat2, corr in high_corr_pairs:
                    if feat1 not in redundant and feat2 not in redundant:
                        # ç§»é™¤ç›¸å…³æ€§è¾ƒä½çš„é‚£ä¸ªï¼ˆä¸labelçš„ç›¸å…³æ€§ï¼‰
                        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç§»é™¤feat2
                        redundant.add(feat2)
                        removed_by_reason['correlation_redundancy'].append({
                            'feature': feat2,
                            'corr_with': feat1,
                            'correlation': corr
                        })
                        removed_by_reason['correlation_pairs'].append({
                            'feat1': feat1,
                            'feat2': feat2,
                            'corr': corr
                        })
                        print(f"  âš ï¸  REDUNDANT: {feat2} (corr={corr:.3f} with {feat1})")

                remove_features.update(redundant)
                valid_numeric = [f for f in valid_numeric if f not in redundant]

            # 3. å•ç‰¹å¾æ€§èƒ½è¯„ä¼°
            print("\n[3/4] Univariate Performance...")

            from sklearn.metrics import roc_auc_score

            feature_scores = {}
            for col in valid_numeric:
                if col in remove_features:
                    continue

                try:
                    feature_data = self.df[col].fillna(0)  # ç®€å•å¡«å……

                    scores = []
                    task_scores = {}
                    for label in self.label_cols:
                        label_data = self.df[label]

                        # äºŒåˆ†ç±»AUC
                        valid_mask = ~(feature_data.isna() | label_data.isna())
                        if valid_mask.sum() > 100:
                            try:
                                auc = roc_auc_score(
                                    label_data[valid_mask],
                                    feature_data[valid_mask]
                                )
                                task_score = abs(auc - 0.5) * 2  # å½’ä¸€åŒ–åˆ°[0,1]
                                scores.append(task_score)
                                task_scores[label] = task_score
                            except:
                                pass

                    if scores:
                        avg_score = np.mean(scores)
                        feature_scores[col] = avg_score
                        feature_scores_by_task[col] = task_scores

                except Exception as e:
                    pass

            # ç§»é™¤ä½åˆ†ç‰¹å¾
            low_score_threshold = 0.05  # ä¸šç•Œæ ‡å‡†
            low_score_features = [f for f, s in feature_scores.items()
                                  if s < low_score_threshold]

            for f in low_score_features:
                removed_by_reason['low_univariate_auc'].append({
                    'feature': f,
                    'auc_score': feature_scores[f]
                })

            print(f"  â„¹ï¸  {len(low_score_features)} features with low univariate score (< {low_score_threshold})")
            remove_features.update(low_score_features)

            # 4. ä¿¡æ¯å€¼ (Information Value) - é£æ§è¡Œä¸šæ ‡å‡†æ–¹æ³•
            print("\n[4/4] Information Value (IV) - Risk Control Standard...")

            iv_threshold = 0.02  # é£æ§è¡Œä¸šæœ€ä½æ ‡å‡†

            for col in valid_numeric:
                if col in remove_features:
                    continue

                try:
                    # å¤šä»»åŠ¡IVï¼šåˆ†åˆ«è®¡ç®—ååšèšåˆï¼ˆmean - alpha * stdï¼‰
                    iv_by_task = {}
                    for label in self.label_cols:
                        label_data = self.df[label]
                        iv_by_task[label] = self._calculate_iv(self.df[col], label_data, n_bins=10)

                    iv_values = list(iv_by_task.values())
                    iv_score = float(np.mean(iv_values) - iv_multitask_alpha * np.std(iv_values))
                    feature_iv_scores[col] = iv_score
                    feature_iv_scores_by_task[col] = iv_by_task

                    # è¾“å‡ºIVåˆ†æ
                    if iv_score < 0.02:
                        remove_features.add(col)
                        removed_by_reason['low_iv_numeric'].append({
                            'feature': col,
                            'iv': iv_score,
                            'iv_by_task': iv_by_task
                        })
                        print(f"  âŒ REMOVE: {col} (IV={iv_score:.4f} - No predictive power)")
                    elif iv_score < 0.1:
                        print(f"  âš ï¸  WEAK: {col} (IV={iv_score:.4f} - Weak predictor)")
                    elif iv_score < 0.3:
                        print(f"  âœ“ MEDIUM: {col} (IV={iv_score:.4f} - Medium predictor)")
                    elif iv_score < 0.5:
                        print(f"  âœ“âœ“ STRONG: {col} (IV={iv_score:.4f} - Strong predictor)")
                    else:
                        print(f"  âš ï¸  SUSPICIOUS: {col} (IV={iv_score:.4f} - Possible data leakage)")

                except Exception as e:
                    pass

            print(f"\n  â„¹ï¸  {len(removed_by_reason['low_iv_numeric'])} numeric features with IV < {iv_threshold}")
        else:
            print("\n  â„¹ï¸  No numeric features to filter")
            feature_scores = {}

        # ============================================================
        # ç±»åˆ«ç‰¹å¾ç­›é€‰
        # ============================================================
        print("\n" + "="*80)
        print("[CATEGORICAL FEATURES FILTERING]")
        print("="*80)

        if categorical_features:
            print("\n[1/1] WoE + IV (Weight of Evidence) - Risk Control Standard...")
            print(f"  Processing {len(categorical_features)} categorical features...")

            iv_threshold = 0.02  # é£æ§è¡Œä¸šæœ€ä½æ ‡å‡†

            for col in categorical_features:
                try:
                    # å¤šä»»åŠ¡IVï¼šåˆ†åˆ«è®¡ç®—ååšèšåˆï¼ˆmean - alpha * stdï¼‰
                    iv_by_task = {}
                    for label in self.label_cols:
                        label_data = self.df[label]
                        iv_by_task[label] = self._calculate_woe_iv_categorical(self.df[col], label_data)

                    iv_values = list(iv_by_task.values())
                    iv_score = float(np.mean(iv_values) - iv_multitask_alpha * np.std(iv_values))
                    feature_iv_scores[col] = iv_score
                    feature_iv_scores_by_task[col] = iv_by_task

                    # è¾“å‡ºIVåˆ†æ
                    if iv_score < 0.02:
                        remove_features.add(col)
                        removed_by_reason['low_iv_categorical'].append({
                            'feature': col,
                            'iv': iv_score,
                            'iv_by_task': iv_by_task
                        })
                        print(f"  âŒ REMOVE: {col} (IV={iv_score:.4f} - No predictive power)")
                    elif iv_score < 0.1:
                        print(f"  âš ï¸  WEAK: {col} (IV={iv_score:.4f} - Weak predictor)")
                    elif iv_score < 0.3:
                        print(f"  âœ“ MEDIUM: {col} (IV={iv_score:.4f} - Medium predictor)")
                    elif iv_score < 0.5:
                        print(f"  âœ“âœ“ STRONG: {col} (IV={iv_score:.4f} - Strong predictor)")
                    else:
                        print(f"  âš ï¸  SUSPICIOUS: {col} (IV={iv_score:.4f} - Possible data leakage)")

                except Exception as e:
                    print(f"  âš ï¸  ERROR: {col} - {e}")

            # ç§»é™¤ä½IVç‰¹å¾
            low_iv_categorical = [f for f in categorical_features
                                if f in feature_iv_scores and feature_iv_scores[f] < iv_threshold]
            print(f"\n  â„¹ï¸  {len(low_iv_categorical)} categorical features with IV < {iv_threshold}")

        else:
            print("\n  â„¹ï¸  No categorical features to filter")

        # ============================================================
        # æ±‡æ€»è¾“å‡º
        # ============================================================
        print("\n" + "="*80)
        print("[IV DISTRIBUTION SUMMARY]")
        print("="*80)

        if feature_iv_scores:
            print(f"  Weak (<0.1): {sum(1 for iv in feature_iv_scores.values() if iv < 0.1)}")
            print(f"  Medium (0.1-0.3): {sum(1 for iv in feature_iv_scores.values() if 0.1 <= iv < 0.3)}")
            print(f"  Strong (0.3-0.5): {sum(1 for iv in feature_iv_scores.values() if 0.3 <= iv < 0.5)}")
            print(f"  Suspicious (>0.5): {sum(1 for iv in feature_iv_scores.values() if iv >= 0.5)}")

        self.results['stage3_filter'] = {
            'remove': list(remove_features),
            'feature_scores': feature_scores,
            'feature_scores_by_task': feature_scores_by_task,
            'iv_scores': feature_iv_scores,
            'iv_scores_by_task': feature_iv_scores_by_task,
            'removed_by_reason': removed_by_reason,
            'iv_threshold': iv_threshold,
            'iv_multitask_alpha': iv_multitask_alpha,
            'numeric_count': len(numeric_features),
            'categorical_count': len(categorical_features),
            'sequence_count': len(sequence_features)
        }

        print(f"\nâœ“ Stage 3 Complete: {len(remove_features)} features to remove")

        return {
            'remove': list(remove_features),
            'scores': feature_scores,
            'iv_scores': feature_iv_scores,
            'scores_by_task': feature_scores_by_task,
            'iv_scores_by_task': feature_iv_scores_by_task,
            'removed_by_reason': removed_by_reason
        }

    # ============================================================
    # Stage 4: å¤šä»»åŠ¡ç‰¹å¼‚æ€§åˆ†æ
    # ============================================================
    def stage4_multitask_analysis(self, features: List[str]) -> Dict:
        """
        é˜¶æ®µ4: å¤šä»»åŠ¡ç‰¹å¼‚æ€§åˆ†æ

        åˆ†æå†…å®¹:
        1. ä»»åŠ¡å…±äº«ç‰¹å¾ (Task-Shared Features) - å¯¹æ‰€æœ‰ä»»åŠ¡éƒ½æœ‰ç”¨
        2. ä»»åŠ¡ç‰¹å¼‚ç‰¹å¾ (Task-Specific Features) - åªå¯¹æŸä¸ªä»»åŠ¡æœ‰ç”¨
        3. ä»»åŠ¡å†²çªç‰¹å¾ (Task-Conflicting Features) - å¯¹ä¸åŒä»»åŠ¡æœ‰ç›¸åä½œç”¨

        å‚è€ƒ: Springer (2025) - Deep multi-task learning review
        """
        print("\n" + "="*80)
        print("STAGE 4: MULTI-TASK SPECIFIC ANALYSIS")
        print("="*80)

        if features:
            candidate_features = [f for f in features if f in self.df.columns]
        else:
            candidate_features = [col for col in self.df.columns if col not in self.label_cols]

        from sklearn.metrics import roc_auc_score

        feature_task_importance = {}

        # è®¡ç®—æ¯ä¸ªç‰¹å¾å¯¹æ¯ä¸ªä»»åŠ¡çš„é‡è¦æ€§
        for col in candidate_features:
            if not pd.api.types.is_numeric_dtype(self.df[col]):
                continue

            feature_data = self.df[col].fillna(0)
            importances = {}

            for label in self.label_cols:
                label_data = self.df[label]

                valid_mask = ~(feature_data.isna() | label_data.isna())
                if valid_mask.sum() > 100:
                    try:
                        auc = roc_auc_score(label_data[valid_mask], feature_data[valid_mask])
                        importances[label] = abs(auc - 0.5) * 2
                    except:
                        importances[label] = 0

            feature_task_importance[col] = importances

        # åˆ†ç±»ç‰¹å¾
        task_specific = {}
        task_shared = []
        task_conflicting = []

        for feat, imps in feature_task_importance.items():
            if len(imps) < len(self.label_cols):
                continue

            values = list(imps.values())
            max_val = max(values)
            min_val = min(values)

            # ä»»åŠ¡ç‰¹å¼‚: æŸä¸ªä»»åŠ¡çš„é‡è¦æ€§æ˜¾è‘—é«˜äºå…¶ä»–ä»»åŠ¡
            if max_val > 2 * np.mean(values) and max_val > 0.3:
                dominant_task = max(imps, key=imps.get)
                task_specific[feat] = {
                    'dominant_task': dominant_task,
                    'importance': imps
                }

            # ä»»åŠ¡å†²çª: ä¸åŒä»»åŠ¡çš„é‡è¦æ€§ç¬¦å·ç›¸åï¼ˆéœ€è¦åŸå§‹ç›¸å…³æ€§ç¬¦å·ï¼‰
            # è¿™é‡Œç®€åŒ–å¤„ç†

            # ä»»åŠ¡å…±äº«: å¯¹æ‰€æœ‰ä»»åŠ¡éƒ½æœ‰ä¸­ç­‰ä»¥ä¸Šé‡è¦æ€§
            elif min_val > 0.1:
                task_shared.append(feat)

        # è¾“å‡ºç»“æœ
        print(f"\n[Summary]")
        print(f"  Task-Specific Features: {len(task_specific)}")
        print(f"  Task-Shared Features: {len(task_shared)}")

        # Top task-specific features
        print(f"\n[Top 10 Task-Specific Features]")
        for feat, info in sorted(task_specific.items(),
                                key=lambda x: x[1]['importance'][x[1]['dominant_task']],
                                reverse=True)[:10]:
            print(f"  {feat}")
            for task, imp in info['importance'].items():
                print(f"    - {task}: {imp:.4f}")

        self.results['stage4_multitask'] = {
            'task_specific': task_specific,
            'task_shared': task_shared,
            'task_conflicting': task_conflicting
        }

        print(f"\nâœ“ Stage 4 Complete")

        return {
            'task_specific': task_specific,
            'task_shared': task_shared,
            'task_conflicting': task_conflicting
        }

    # ============================================================
    # Stage 5: æ¨¡å‹æ–¹æ³• (Embedded Methods)
    # ============================================================
    def stage5_model_based_selection(self, features: List[str], top_k: int = 100, use_categorical: bool = True) -> Dict:
        """
        é˜¶æ®µ5: åŸºäºæ¨¡å‹çš„ç‰¹å¾ç­›é€‰ï¼ˆæ”¯æŒæ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾ï¼‰

        æ–¹æ³•:
        1. LightGBMç‰¹å¾é‡è¦æ€§ - æ”¯æŒæ•°å€¼å’Œç±»åˆ«ç‰¹å¾
        2. Label Encoding for ç±»åˆ«ç‰¹å¾
        3. Top-Kç‰¹å¾é€‰æ‹©

        å‚è€ƒ: Cambridge (2024) - LASSO for multitask feature selection
        """
        print("\n" + "="*80)
        print("STAGE 5: MODEL-BASED SELECTION (Numeric + Categorical)")
        print("="*80)

        if features:
            candidate_features = [f for f in features if f in self.df.columns]
        else:
            candidate_features = [col for col in self.df.columns if col not in self.label_cols]

        # ç‰¹å¾åˆ†ç±»
        numeric_features = []
        categorical_features = []
        sequence_features = []

        for col in candidate_features:
            if self._is_sequence_feature(col):
                sequence_features.append(col)
            elif pd.api.types.is_numeric_dtype(self.df[col]):
                numeric_features.append(col)
            else:
                categorical_features.append(col)

        print(f"\n[Feature Type Distribution]")
        print(f"  Numeric: {len(numeric_features)}")
        print(f"  Categorical: {len(categorical_features)}")
        print(f"  Sequence (skipped): {len(sequence_features)}")

        print("\n[1/3] Preparing data for LightGBM...")

        try:
            import lightgbm as lgb
            from sklearn.preprocessing import LabelEncoder
            aggregation_beta = 0.2

            # å‡†å¤‡æ•°å€¼ç‰¹å¾
            X_numeric = self.df[numeric_features].fillna(0) if numeric_features else pd.DataFrame()

            # å‡†å¤‡ç±»åˆ«ç‰¹å¾ï¼ˆLabel Encodingï¼‰
            X_categorical = pd.DataFrame(index=self.df.index)
            le_dict = {}

            for col in categorical_features:
                le = LabelEncoder()
                # å¤„ç†NaNå€¼ï¼šå¡«å……ä¸º'UNKNOWN'åå†ç¼–ç 
                X_categorical[col] = le.fit_transform(
                    self.df[col].fillna('UNKNOWN').astype(str)
                )
                le_dict[col] = le
                print(f"  Encoded {col}: {len(le.classes_)} unique values")

            # åˆå¹¶ç‰¹å¾
            X_list = []
            if not X_numeric.empty:
                X_list.append(X_numeric)
            if not X_categorical.empty:
                X_list.append(X_categorical)

            if not X_list:
                print("  âš ï¸  No features available for model-based selection")
                return {'top_features': [], 'importance': {}, 'categorical_features': []}

            X = pd.concat(X_list, axis=1)

            print(f"\n[2/3] Training LightGBM with {X.shape[1]} features...")

            params = {
                'objective': 'binary',
                'verbose': -1,
                'num_leaves': 31,
                'learning_rate': 0.05,
                'n_estimators': 100,
                'num_threads': 1,
                'force_col_wise': True,
                'deterministic': True,
                'min_data_per_group': 1,
                'cat_l2': 10,
                'cat_smooth': 10
            }

            per_task_importance = {}
            for label in self.label_cols:
                y = self.df[label].fillna(0)
                if y.nunique(dropna=True) < 2:
                    print(f"  âš ï¸  Skip task {label}: label has < 2 classes")
                    continue

                train_data = lgb.Dataset(
                    X,
                    label=y,
                    categorical_feature=(categorical_features if (use_categorical and categorical_features) else 'auto')
                )
                model = lgb.train(params, train_data)
                importance = model.feature_importance(importance_type='gain')
                per_task_importance[label] = dict(zip(X.columns, importance))

            if not per_task_importance:
                raise ValueError("no valid task labels for stage5 model training")

            # èšåˆä»»åŠ¡é‡è¦æ€§: mean - beta * stdï¼Œåå‘ç¨³å®šå…±äº«ç‰¹å¾
            feature_importance = {}
            for feature in X.columns:
                task_vals = [
                    task_imp.get(feature, 0.0)
                    for task_imp in per_task_importance.values()
                ]
                mean_imp = float(np.mean(task_vals))
                std_imp = float(np.std(task_vals))
                feature_importance[feature] = mean_imp - aggregation_beta * std_imp

            # æ’åºå¹¶é€‰æ‹©top-k
            sorted_features = sorted(feature_importance.items(),
                                    key=lambda x: x[1], reverse=True)

            top_features = [f for f, _ in sorted_features[:top_k]]

            print(f"\n[3/3] Top 10 Features by LightGBM Importance]")
            for i, (feat, imp) in enumerate(sorted_features[:10], 1):
                feat_type = "ğŸ”¢" if feat in numeric_features else "ğŸ“"
                print(f"  {i:2}. {feat_type} {feat}: {imp}")

            # ç»Ÿè®¡top_kä¸­çš„ç‰¹å¾ç±»å‹åˆ†å¸ƒ
            numeric_in_top = sum(1 for f in top_features if f in numeric_features)
            categorical_in_top = sum(1 for f in top_features if f in categorical_features)

            print(f"\n[Top-K Feature Distribution]")
            print(f"  Numeric: {numeric_in_top}/{len(numeric_features)} available")
            print(f"  Categorical: {categorical_in_top}/{len(categorical_features)} available")

            self.results['stage5_model'] = {
                'top_features': top_features,
                'feature_importance': feature_importance,
                'per_task_importance': per_task_importance,
                'use_categorical': use_categorical,
                'importance_aggregation': f'mean - {aggregation_beta} * std',
                'numeric_features': numeric_features,
                'categorical_features': categorical_features,
                'encoding': 'label_encoding'
            }

            print(f"\nâœ“ Stage 5 Complete: Selected top {len(top_features)} features")

            return {
                'top_features': top_features,
                'importance': feature_importance,
                'per_task_importance': per_task_importance,
                'use_categorical': use_categorical,
                'categorical_features': categorical_features
            }

        except ImportError:
            print("  âš ï¸  LightGBM not installed, skipping model-based selection")
            # è¿”å›ç®€å•çš„é€‰æ‹©ï¼ˆæ•°å€¼ç‰¹å¾ä¼˜å…ˆï¼‰
            all_features = numeric_features + categorical_features
            return {
                'top_features': all_features[:top_k],
                'importance': {},
                'categorical_features': categorical_features
            }
        except Exception as e:
            print(f"  âš ï¸  Error in Stage 5: {e}")
            # è¿”å›ç®€å•çš„é€‰æ‹©
            all_features = numeric_features + categorical_features
            return {
                'top_features': all_features[:top_k],
                'importance': {},
                'categorical_features': categorical_features
            }

    # ============================================================
    # Stage 6: ç‰¹å¾ç¨³å®šæ€§éªŒè¯
    # ============================================================
    def stage6_stability_validation(self, features: List[str]) -> Dict:
        """
        é˜¶æ®µ6: ç‰¹å¾ç¨³å®šæ€§éªŒè¯

        æ£€æŸ¥å†…å®¹:
        1. æ—¶é—´ç¨³å®šæ€§ - ä¸åŒæ—¶é—´æ®µç‰¹å¾åˆ†å¸ƒæ˜¯å¦ç¨³å®š
        2. æ ·æœ¬ç¨³å®šæ€§ - ä¸åŒé‡‡æ ·ä¸‹ç‰¹å¾è¡¨ç°æ˜¯å¦ä¸€è‡´

        ä¸šç•Œå®è·µ: å‚è€ƒé˜¿é‡Œæ¨èç³»ç»Ÿç‰¹å¾ç¨³å®šæ€§ç›‘æ§
        """
        print("\n" + "="*80)
        print("STAGE 6: STABILITY VALIDATION")
        print("="*80)

        # ç®€åŒ–å®ç°: ä½¿ç”¨bootstrapé‡‡æ ·éªŒè¯ç¨³å®šæ€§
        print("\n[1/1] Bootstrap Stability Check...")

        if features:
            candidate_features = [f for f in features if f in self.df.columns]
        else:
            candidate_features = [col for col in self.df.columns if col not in self.label_cols]

        numeric_features = [col for col in candidate_features
                          if pd.api.types.is_numeric_dtype(self.df[col])]

        n_bootstrap = 5
        sample_size = min(100000, len(self.df) // 2)

        feature_stability = {}

        for col in numeric_features:
            aucs = []

            for i in range(n_bootstrap):
                # é‡‡æ ·
                sample_df = self.df.sample(n=sample_size, replace=True)

                feature_data = sample_df[col].fillna(0)
                label_data = sample_df[self.label_cols[0]]

                valid_mask = ~(feature_data.isna() | label_data.isna())
                if valid_mask.sum() > 100:
                    try:
                        from sklearn.metrics import roc_auc_score
                        auc = roc_auc_score(label_data[valid_mask], feature_data[valid_mask])
                        aucs.append(abs(auc - 0.5) * 2)
                    except:
                        pass

            if aucs:
                stability = np.std(aucs)  # æ ‡å‡†å·®è¶Šå°è¶Šç¨³å®š
                feature_stability[col] = {
                    'mean_auc': np.mean(aucs),
                    'std_auc': stability
                }

        # æ‰¾å‡ºä¸ç¨³å®šç‰¹å¾
        unstable = [f for f, s in feature_stability.items() if s['std_auc'] > 0.1]

        print(f"\n[Unstable Features (std > 0.1)]")
        for feat in unstable[:10]:
            print(f"  âš ï¸  {feat}: std={feature_stability[feat]['std_auc']:.4f}")

        self.results['stage6_stability'] = {
            'stability': feature_stability,
            'unstable': unstable
        }

        print(f"\nâœ“ Stage 6 Complete: {len(unstable)} unstable features")

        return {'stability': feature_stability, 'unstable': unstable}

    # ============================================================
    # Stage 7: ä¸šåŠ¡é€»è¾‘å®¡æŸ¥
    # ============================================================
    def stage7_domain_review(self, features: List[str]) -> Dict:
        """
        é˜¶æ®µ7: ä¸šåŠ¡é€»è¾‘å®¡æŸ¥

        æ£€æŸ¥é¡¹:
        1. ç‰¹å¾è®¡ç®—é€»è¾‘æ˜¯å¦åˆç†
        2. ç‰¹å¾æ˜¯å¦åŒ…å«æœªæ¥ä¿¡æ¯
        3. ç‰¹å¾æ˜¯å¦å¯åœ¨çº¿ä¸Šå®æ—¶è®¡ç®—

        ä¸šç•Œå®è·µ: å¿…é¡»ç”±ä¸šåŠ¡ä¸“å®¶review
        """
        print("\n" + "="*80)
        print("STAGE 7: DOMAIN REVIEW")
        print("="*80)

        print("\nâš ï¸  This stage requires manual domain expert review!")

        # ç”Ÿæˆéœ€è¦äººå·¥å®¡æŸ¥çš„ç‰¹å¾åˆ—è¡¨
        review_features = []

        if features:
            candidate_features = [f for f in features if f in self.df.columns]
        else:
            candidate_features = [col for col in self.df.columns if col not in self.label_cols]

        # ç‰¹å¾åˆ†ç±»
        model_features = [f for f in candidate_features if 'model' in f.lower()]
        rate_features = [f for f in candidate_features if 'rate' in f.lower()]
        tag_features = [f for f in candidate_features if f.endswith('_tag')]

        print(f"\n[Features Requiring Review]")
        print(f"  Model-related: {len(model_features)}")
        print(f"  Rate-related: {len(rate_features)}")
        print(f"  Tag features: {len(tag_features)}")

        # ä¿å­˜å¾…å®¡æŸ¥åˆ—è¡¨
        review_file = self.output_dir / "features_for_domain_review.csv"
        pd.DataFrame({'feature': candidate_features}).to_csv(review_file, index=False)

        print(f"\n  Saved review list to: {review_file}")

        self.results['stage7_domain'] = {
            'review_required': len(candidate_features),
            'categories': {
                'model': len(model_features),
                'rate': len(rate_features),
                'tag': len(tag_features)
            }
        }

        return {'review_required': True}

    # ============================================================
    # å®Œæ•´æµç¨‹æ‰§è¡Œ
    # ============================================================
    def run_full_pipeline(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„ç‰¹å¾ç­›é€‰æµç¨‹"""

        print("\n" + "="*80)
        print("MULTI-TASK FEATURE SELECTION PIPELINE")
        print("="*80)

        # Stage 1: æ•°æ®è´¨é‡æ£€æŸ¥
        stage1_result = self.stage1_data_quality_check()
        remove_set1 = set(stage1_result['remove'])

        # Stage 2: æ•°æ®æ³„éœ²æ£€æµ‹ - SKIPPED per user request
        # These features may be useful in production if they can be obtained at inference time
        print("\n" + "="*80)
        print("STAGE 2: DATA LEAKAGE DETECTION - SKIPPED")
        print("="*80)
        print("\nâ„¹ï¸  Stage 2 skipped per user request.")
        print("    If using model-output features (reportmodel_*), ensure they are")
        print("    obtainable at inference time.")

        stage2_result = {'leakage': []}
        remove_set2 = set()

        # å‰©ä½™ç‰¹å¾
        remaining_features = [col for col in self.df.columns
                             if col not in self.label_cols
                             and col not in remove_set1
                             and col not in remove_set2]

        # Stage 3: Filteræ–¹æ³•
        stage3_result = self.stage3_filter_methods(remaining_features)
        remove_set3 = set(stage3_result['remove'])

        # æ›´æ–°å‰©ä½™ç‰¹å¾
        remaining_features = [f for f in remaining_features if f not in remove_set3]

        # Stage 4: å¤šä»»åŠ¡åˆ†æ
        stage4_result = self.stage4_multitask_analysis(remaining_features)

        # Stage 5: æ¨¡å‹æ–¹æ³•
        stage5_result = self.stage5_model_based_selection(remaining_features, top_k=150)

        # Stage 6: ç¨³å®šæ€§éªŒè¯
        # stage6_result = self.stage6_stability_validation(stage5_result['top_features'])

        # Stage 7: ä¸šåŠ¡å®¡æŸ¥
        # stage7_result = self.stage7_domain_review(stage5_result['top_features'])

        # æ±‡æ€»æœ€ç»ˆç‰¹å¾åˆ—è¡¨
        final_features = stage5_result['top_features']

        # ä¿å­˜ç»“æœ
        self.results['final_features'] = final_features

        output_file = self.output_dir / "feature_selection_results.json"
        with open(output_file, 'w') as f:
            # è½¬æ¢numpyç±»å‹
            def convert(obj):
                if isinstance(obj, (np.integer, np.floating)):
                    return float(obj)
                elif isinstance(obj, list):
                    return [convert(x) for x in obj]
                elif isinstance(obj, dict):
                    return {k: convert(v) for k, v in obj.items()}
                return obj

            json.dump(convert(self.results), f, indent=2)

        # ä¿å­˜æœ€ç»ˆç‰¹å¾åˆ—è¡¨
        final_features_file = self.output_dir / "final_features.txt"
        with open(final_features_file, 'w') as f:
            for feat in final_features:
                f.write(f"{feat}\n")

        print("\n" + "="*80)
        print("PIPELINE COMPLETE")
        print("="*80)
        print(f"\nFinal feature count: {len(final_features)}")
        print(f"Results saved to: {output_file}")
        print(f"Feature list saved to: {final_features_file}")

        return self.results


# ============================================================
# å‘½ä»¤è¡Œå…¥å£
# ============================================================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Multi-Task Feature Selection Pipeline')
    parser.add_argument('--data', type=str,
                       default="/mnt/home/gxwang9/fuxictr/data/all_seeds_1v5_rh_0206/train.parquet",
                       help='Path to training data')
    parser.add_argument('--labels', type=str, nargs='+',
                       default=['label_register', 'label_apply', 'label_credit'],
                       help='Label columns')
    parser.add_argument('--output', type=str,
                       default="/mnt/home/gxwang9/fuxictr/analysis/feature_selection_output",
                       help='Output directory')
    parser.add_argument('--stage', type=str, default='all',
                       choices=['all', '1', '2', '3', '4', '5', '6', '7'],
                       help='Which stage to run (default: all)')

    args = parser.parse_args()

    # åˆ›å»ºpipeline
    pipeline = MultiTaskFeatureSelectionPipeline(
        data_path=args.data,
        label_cols=args.labels,
        output_dir=args.output
    )

    # æ‰§è¡Œ
    if args.stage == 'all':
        results = pipeline.run_full_pipeline()
    else:
        # è¿è¡Œå•ä¸ªstage
        stage_methods = {
            '1': pipeline.stage1_data_quality_check,
            '2': lambda: pipeline.stage2_leakage_detection(threshold=0.8),
            '3': lambda: pipeline.stage3_filter_methods(None),
            '4': lambda: pipeline.stage4_multitask_analysis(None),
            '5': lambda: pipeline.stage5_model_based_selection(None, top_k=150),
            '6': lambda: pipeline.stage6_stability_validation(None),
            '7': lambda: pipeline.stage7_domain_review(None),
        }
        results = stage_methods[args.stage]()

    print("\nâœ“ Pipeline execution complete!")
